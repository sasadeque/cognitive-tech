{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "import json\n",
    "import os \n",
    "import urllib.request\n",
    "import datetime\n",
    "import boto3\n",
    "from collections import Counter\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    " \n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import simplejson as json\n",
    " \n",
    "from nlp import build_model\n",
    " \n",
    "def process_str(s):\n",
    "     s = s.translate(None, string.punctuation)\n",
    "    return word_tokenize(s.lower())\n",
    " \n",
    "def read_dataset(sentences, labels):\n",
    "     dataset = []\n",
    "    with open(sentences) as sentences, open(labels) as labels:\n",
    "        import ipdb; ipdb.set_trace()\n",
    "        for s, l in zip(sentences, labels):\n",
    "            try:\n",
    "                words = process_str(s)\n",
    "                dataset.append( (int(l), set(words)) )\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return dataset\n",
    " \n",
    "def get_most_commons(dataset, skip=10, total=4000):\n",
    "    my_list = []\n",
    "    for item in dataset:\n",
    "        my_list += list(item[1])\n",
    "\n",
    "           counter = Counter(my_list)\n",
    "\n",
    "               temp = counter.most_common(total+skip)[skip:]\n",
    "    words = [item[0] for item in temp]\n",
    "    return words\n",
    "def generate_vectors(dataset, common_words, ternary, bias_term):\n",
    "    d = {}\n",
    "    for i in range(len(common_words)):\n",
    "        d[common_words[i]] = i\n",
    "         vectors = []\n",
    "    for item in dataset:\n",
    "        vector = [0] * len(common_words)\n",
    "        for word in item[1]:\n",
    "             if word in d:\n",
    "                vector[d[word]] += 1\n",
    "\n",
    "            if bias_term:\n",
    "            vector.append(1.0)\n",
    " \n",
    "         vectors.append( (item[0], np.array(vector)) )\n",
    " \n",
    "   return vectors\n",
    "\n",
    "def random_sample(data, proportion):\n",
    "\n",
    "    count = int(len(data) * proportion)\n",
    "    np.random.shuffle(data)\n",
    "    return data[:count]\n",
    "\n",
    "\n",
    "def evaluate(preds, golds):\n",
    "    tp, pp, cp = 0.0, 0.0, 0.0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        if pred == 1:\n",
    "            pp += 1\n",
    "        if gold == 1:\n",
    "            cp += 1\n",
    "        if pred == 1 and gold == 1:\n",
    "            tp += 1\n",
    "    precision = tp / pp\n",
    "    recall = tp / cp\n",
    "    try:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        return (precision, recall, 0.0)\n",
    "    return (precision, recall, f1)\n",
    " \n",
    " \n",
    " def build_dataset(sentences, labels, nb_words=500):\n",
    "        with open(sentences) as f:\n",
    "            data = f.readlines()\n",
    "        with open(labels) as g:\n",
    "            labels = [int(label) for label in g.read()[:-1].split(\"\\n\")]\n",
    " \n",
    "    ternary = True\n",
    " \n",
    "     train_data = [set(process_str(s)) for s in data]\n",
    "          train_data = [(label, data) for label, data in zip(labels, train_data)]\n",
    " \n",
    "     common_words = get_most_commons(train_data, total=nb_words)\n",
    " \n",
    "     train_vectors = generate_vectors(train_data, common_words, ternary, False)\n",
    " \n",
    "     X = np.array([e[1] for e in train_vectors])\n",
    "     Y = np.array([e[0] for e in train_vectors]).reshape(-1, 1)\n",
    " \n",
    "    data = np.append(X, Y, axis=1)\n",
    "     np.random.shuffle(data)\n",
    " \n",
    "     X = data[:, :-1]\n",
    "     Y = data[:, -1].reshape(-1, 1)\n",
    " \n",
    "     return X, Y, common_words\n",
    " \n",
    " \n",
    " if __name__ == '__main__':\n",
    " \n",
    "    batch_size = 128\n",
    "    nb_classes = 2\n",
    "     nb_epoch = 12\n",
    "         nb_words = 4000\n",
    " \n",
    "     X, Y, common_words = build_dataset('sentences.txt', 'labels.txt', nb_words=nb_words)\n",
    " \n",
    "     model = build_model(nb_words)\n",
    "     model.fit(X[:7000], Y[:7000], nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "         score = model.evaluate(X[7000:], Y[7000:])\n",
    "             print('Loss: ', score[0], ' Accuracy: ', score[1])\n",
    " \n",
    "     with open('parsed_chat.json') as f:\n",
    "         chat_file = json.load(f)\n",
    " \n",
    "     messages = [message['message'] for message in chat_file['threads'][0]['messages'] if message['sender'] == '']\n",
    "     message = [(0, ' '.join(messages))]\n",
    "    import ipdb; ipdb.set_trace()\n",
    "     message_vector = generate_vectors(message, common_words, False, False)\n",
    "         X = np.array([message_vector[0][1]])\n",
    "             print(model.predict(X))\n",
    "\n",
    "class StreamHandler():\n",
    "    def __init__(self):\n",
    "        self.fb_access_token = None\n",
    "        self.fb_access_token_secret = None\n",
    "        self.fb_consumer_key = None\n",
    "        self.fb_consumer_secret = None\n",
    "        self.fb_filters_list = None\n",
    "        self.fb_languages_list = None\n",
    "\n",
    "    def FetchLastPictures(self):\n",
    "        stream_callback = FetchedPictureHandler()\n",
    "        auth = fb.OAuthHandler(self.fb_consumer_key, self.fb_consumer_secret)\n",
    "        auth.set_access_token(self.fb_access_token, self.fb_access_token_secret)  \n",
    "        stream = fb.Stream(auth, stream_callback,timeout=120)\n",
    "        stream.filter(languages=self.fb_languages_list ,track=self.fb_filters_list)\n",
    "\n",
    "\n",
    "class FetchedPictureHandler(fb.StreamListener):\n",
    "    def on_data(self, data):\n",
    "        global NUMBER_OF_FETCHED_PICTURES\n",
    "        if NUMBER_OF_FETCHED_PICTURES < NUMBER_OF_PICTURE_TO_FECTH:\n",
    "            try:\n",
    "                decoded = json.loads(data)\n",
    "                if 'extended_entities' in decoded:\n",
    "                    for media in decoded['extended_entities']['media']:\n",
    "                        pic_url = media['media_url_https']\n",
    "                        pic_name = '{}{}_{}'.format(IMG_OUTPUT_LOCATION, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S%f\"), os.path.basename(pic_url))\n",
    "                        urllib.request.urlretrieve(pic_url, pic_name)\n",
    "            \n",
    "                        #check if is there a face within the picture\n",
    "                        p = open(pic_name, 'rb')\n",
    "                        reko_response = rekognition_client_connection.detect_faces(Image={\n",
    "                                                                            'Bytes':bytearray(p.read())\n",
    "                                                                        },\n",
    "                                                                            Attributes=[\n",
    "                                                                            'ALL'\n",
    "                                                                        ])\n",
    "                        p.close()\n",
    "                        if 'FaceDetails' in reko_response and len(reko_response['FaceDetails'])>0:\n",
    "                            for detail in reko_response['FaceDetails']:\n",
    "                                #store details\n",
    "                                if 'Smile' in detail:\n",
    "                                    SMILES.append(detail['Smile']['Value'])\n",
    "                                if 'Gender' in detail:\n",
    "                                    GENDERS.append(detail['Gender']['Value'])\n",
    "\n",
    "                                if 'Emotions' in detail:\n",
    "                                    for e in detail['Emotions']:\n",
    "                                        if e['Type'] not in EMOTIONS:\n",
    "                                            EMOTIONS[e['Type']] = []\n",
    "                                        EMOTIONS[e['Type']].append(e['Confidence'])\n",
    "\n",
    "                            NUMBER_OF_FETCHED_PICTURES = NUMBER_OF_FETCHED_PICTURES + 1\n",
    "                        else:\n",
    "                            os.remove(pic_name)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "def get_text_sentiment(text):\n",
    "\n",
    "    alchemy_api_key = os.environ.get(\"ALCHEMY_API_KEY\")\n",
    "    alchemy_language = AlchemyLanguage(api_key=alchemy_api_key)\n",
    "    result = alchemy_language.sentiment(text=text)\n",
    "    if result['docSentiment']['type'] == 'neutral':\n",
    "        return 'netural', 0\n",
    "        return result['docSentiment']['type'], result['docSentiment']['score']\n",
    "            def on_error(self, status):\n",
    "        # TODO maybe add some log?\n",
    "            print(status)\n",
    "\n",
    "AWS_ACCESS_KEY = ''\n",
    "AWS_SECRET_ACCESS_KEY =''\n",
    "rekognition_client_connection = boto3.client(\n",
    "    'rekognition',\n",
    "    aws_access_key_id = AWS_ACCESS_KEY,\n",
    "    aws_secret_access_key = AWS_SECRET_ACCESS_KEY,\n",
    "    region_name = 'eu-west-1'\n",
    ")\n",
    "\n",
    "\n",
    "NUMBER_OF_FETCHED_PICTURES = 0\n",
    "NUMBER_OF_PICTURE_TO_FECTH = 25 # number of iamges to download\n",
    "IMG_OUTPUT_LOCATION = 'img_out/' # output folder\n",
    "\n",
    "SMILES = []\n",
    "GENDERS = []\n",
    "EMOTIONS = {}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        fb_handler = StreamHandler()\n",
    "        fb_handler.fb_access_token = \"\"\n",
    "        fb_handler.fb_access_token_secret = \"\"\n",
    "        fb_handler.fb_consumer_key = \"\"\n",
    "        fb_handler.fb_consumer_secret = \"\"\n",
    "        fb_handler.fb_filters_list = ['#christmas', '#family', '#friends', '#love', '#faces'] # set of filters\n",
    "        fb_handler.fb_languages_list = ['en']\n",
    "        fb_handler.FetchLastPictures()\n",
    "\n",
    "\t\t# print the output\n",
    "        print(\"SMILING:\")\n",
    "        for i,j in zip(Counter(SMILES).keys(), Counter(SMILES).values()):\n",
    "            print('{} : {}'.format(i, j))\n",
    "        \n",
    "        print(\"GENDER:\")\n",
    "        for i,j in zip(Counter(GENDERS).keys(), Counter(GENDERS).values()):\n",
    "            print('{} : {}'.format(i, j))\n",
    "        \n",
    "        print(\"EMOTIONS:\")\n",
    "        for key, value in EMOTIONS.items():\n",
    "            print('{} : {}'.format(key, sum(value) / float(len(value))))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
